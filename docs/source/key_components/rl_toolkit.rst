RL Toolkit
==========

MARO provides a full-stack abstraction for reinforcement learning (RL) which includes various customizable
components. In order to provide a gentle introduction for the RL toolkit, we cover the components in top-down
fashion, starting from the outermost layer, the learning workflows. The RL toolkit supports single-threaded and
distributed learning workflows. The distributed workflow can be synchronous or asynchronous.


Synchronous Learning
====================

In synchronous mode, a central controller executes learning cycles that consist of simulation data collection and
policy update. In a strictly synchronous learning process, a roll-out manager waits for all data collectors,
a.k.a. "roll-out workers", to return their results before moving on to the policy update phase. So what if a slow
worker drags the whole process down? We provide the flexibility to loosen the restriction by specifying the
number of workers required to report back before proceeding to the next phase. After the required number of workers
report back, the roll-out manager can optionally try to receive from the remaining workers a certain number of times
but with a timeout to keep the wait time upperbounded. Note that the transition from the policy update phase to the
data collection phase is always synchronous. This means that in the case of the policy instances distributed amongst
a set of trainer nodes, the central controller waits until all trainers report back with the latest policy states before
starting the next cycle.


The components required for synchronous learning include:

* Learner, which is the central coordinator for a learning process. The learner consists of a roll-out manager and
  a training manager and executes learning cycles that alternate between data collection and policy update.
* Rollout manager, which is responsible for collecting simulation data, in local or distributed fashion.
* Policy manager, which controls the policy update phase of a learning cycle. See "Policy Manager" below for details.


.. image:: ../images/rl/learner.svg
   :target: ../images/rl/learner.svg
   :alt: Overview


.. image:: ../images/rl/rollout_manager.svg
   :target: ../images/rl/rollout_manager.svg
   :alt: Overview


Asynchronous Learning
=====================

The asynchronous mode consists of a policy server and multiple actors. No central controller is present. Each data collector,
a.k.a., actor, operates independently. As soon as the server receives data from an actor, it feeds the data to the policy
manager for perform updates. The server then returns the updated policy states to the actor for the next round of data collection.
As the actors may differ significantly in speed, the policy server only uses data generated by policies sufficiently up-to-date.
but always sends the latest policy states to every single actor.

The components required for asynchronous learning include:

* actor, which alternates between sending collected simulation data to the policy server and receiving updated 
  policy states from it.
* policy server, which receives data from the actors and update the policy states when necessary.


Environment Sampler
-------------------

It is necessary to implement an environment sampler (a subclass of ``AbsEnvSampler``) with user-defined state, action
and reward shaping to collect roll-out information for learning and testing purposes. An environment sampler can be
easily turned into a roll-out worker or an actor for synchronous and asynchronous learning, respectively.


Policy
------

The ``AbsPolicy`` abstraction is the core component of MARO's RL toolkit. A policy is a an agent's mechanism to select
actions in its interaction with an environment. MARO allows arbitrary agent-to-policy mapping to make policy sharing
easily configurable. It is also possible to use a mixture of rule-based policies and ``RLPolicy`` instances. The latter
provides various policy improvement interfaces to support single-threaded and distributed learning.   


.. code-block:: python

  class AbsPolicy(ABC):
      @abstractmethod
      def __call__(self, state):
          """Select an action based on a state."""
          raise NotImplementedError


  class RLPolicy(AbsPolicy):     
      ...

      def record(self, key: str, state, action, reward, next_state, terminal: bool):
          pass

      def get_rollout_info(self):
          pass

      def get_batch_loss(self, batch: dict, explicit_grad: bool = False):
          pass

      def update(self, loss_info_list: List[dict]):
          pass

      def learn(self, batch: dict):
          pass

      def improve(self):
          pass


Policy Manager
--------------

A policy manager controls policy update and provides the latest policy states for roll-outs. In synchrounous learning,
the policy manager controls the policy update phase of a learning cycle. In asynchrounous learning, the policy manager
is present as a server process. The types of policy manager include:

* ``SimplePolicyManager``, where all policies instances reside within the manager and are updated sequentially;
* ``MultiProcessPolicyManager``, where each policy is placed in a dedicated process that runs event loops to receive
  roll-out information for update; this approach takes advantage of the parallelism from Python's multi-processing, so
  the policies can be updated in parallel, but comes with inter-process communication overhead;
* ``DistributedPolicyManager``, where policies are distributed among a set of remote compute nodes that run event loops
  to reeeive roll-out information for update. This approach allows the policies to be updated in parallel and may be
  necessary when the combined size of the policies is too big to fit in a single node. 


.. image:: ../images/rl/policy_manager.svg
    :target: ../images/rl/policy_manager.svg
    :alt: RL Overview


Core Model
----------

In the deep reinforcement learning (DRL) world, a policy usually includes one or more neural-network com-based models,
which may be used to compute action preferences or estimate state / action values. The ``AbsCoreModel`` represents a
collection of network components with embedded optimizers and exposes unified interfaces to decouple model inference
and optimization from the algorithmic aspects of the policy that uses them. For example, the actor-critic algorithm
does not need to concern itself with how the action probabilities and state values are computed. Subclasses of
``AbsCoreModel`` provided for use with specific RL algorithms include ``DiscreteQNet`` for DQN, ``DiscretePolicyNet``
for Policy Gradient, ``DiscreteACNet`` for Actor-Critic and ``ContinuousACNet`` for DDPG.

The code snippet below shows how to create a model for the actor-critic algorithm with a shared bottom stack:

.. code-block:: python

  shared_net_conf = {...}
  actor_net_conf = {...}
  critic_net_conf = {...}
  shared_optim_conf = {torch.optim.SGD, {"lr": 0.0001}}
  actor_optim_conf = (torch.optim.Adam, {"lr": 0.001})
  critic_optim_conf = (torch.optim.RMSprop, {"lr": 0.001})

  class MyACNet(DiscreteACNet):
      def __init__(self):
          super().__init__()
          self.shared = FullyConnected(**shared_net_conf)
          self.actor = FullyConnected(**actor_net_conf)
          self.critic = FullyConnected(**critic_net_conf)
          self.shared_optim = shared_optim_conf[0](self.shared.parameters(), **shared_optim_conf[1])
          self.actor_optim = actor_optim_conf[0](self.actor.parameters(), **actor_optim_conf[1])
          self.critic_optim = critic_optim_conf[0](self.critic.parameters(), **critic_optim_conf[1])

      def forward(self, states, actor: bool = True, critic: bool = True):
          representation = self.shared(states)
          return (self.actor(representation) if actor else None), (self.critic(representation) if critic else None)

      def step(self, loss):
          self.shared_optim.zero_grad()
          self.actor_optim.zero_grad()
          self.critic_optim.zero_grad()
          loss.backward()
          self.hsared_optim.step()
          self.actor_optim.step()
          self.critic_optim.step()

To generate stochastic actions given a batch of states, call ``get_action`` on the model instance: 

.. code-block:: python

  action, log_p, values = ac_model.get_action(state)

To performing a single gradient step on the model, pass the loss to the ``step`` function: 

.. code-block:: python

  ac_model.step(critic_loss + actor_loss)
