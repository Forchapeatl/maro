# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

from abc import ABC, abstractmethod

from maro.rl.algorithms import AbsAlgorithm
from maro.rl.experience import ExperienceSet, ExperienceStore, UniformSampler


class AbsPolicy(ABC):
    """Abstract policy class."""
    def __init__(self):
        super().__init__()

    @abstractmethod
    def choose_action(self, state):
        raise NotImplementedError


class NullPolicy(AbsPolicy):
    """Dummy policy that does nothing.

    Note that the meaning of a "None" action may depend on the scenario.
    """
    def choose_action(self, state):
        return None


class AbsCorePolicy(AbsPolicy):
    """Policy that can update itself using simulation experiences.

    Reinforcement learning (RL) policies should inherit from this.

    Args:
        algorithm (AbsAlgorithm): Algorithm instance.
        experience_store (ExperienceStore): An ``ExperienceStore`` instance for storing and retrieving experiences
            generated by the policy.
        experience_sampler_cls: Type of experience sampler. Must be a subclass of ``AbsSampler``. Defaults to
            ``UnifromSampler``.
        experience_sampler_kwargs (dict): Keyword arguments for ``experience_sampler_cls``.
        num_epochs (int): Number of times ``self.algorithm.learn()`` is called in each call to ``update``. Defaults
            to 1.
        update_trigger (int): The required number of new experiences to trigger a call to ``update``. Defaults to 1.
        warmup (int): The minimum number of experiences in the experience memory required to trigger a call to ``update``
            Defaults to 1.
        auto (bool): If true, the policy instance should be updated in the ``learn`` method. Otherwise, the method simply
            returns the necessary update information. The latter is usually used in Defaults to True. 
    """
    def __init__(
        self,
        algorithm: AbsAlgorithm,
        experience_store: ExperienceStore,
        experience_sampler_cls=UniformSampler,
        experience_sampler_kwargs: dict = {},
        num_epochs: int = 1,
        update_trigger: int = 1,
        warmup: int = 1,
        auto: bool = True
    ):
        super().__init__()
        self.algorithm = algorithm
        self.experience_store = experience_store
        self.sampler = experience_sampler_cls(self.experience_store, **experience_sampler_kwargs)
        self.num_epochs = num_epochs
        self.update_trigger = update_trigger
        self.warmup = warmup
        self.tracker = {}
        self.auto = auto

        self._exp_cache = ExperienceSet()
        self._new_exp_counter = 0

    def choose_action(self, state):
        return self.algorithm.choose_action(state)

    def update(self, experience_batch):
        """Policy update logic is implemented here.

        This usually includes retrieving experiences as training samples from the experience manager and
        updating the underlying models using these samples.
        """
        if self.auto:
            if self._new_exp_counter >= self.update_trigger and self.experience_store.size >= self.warmup:
                experience_batch = self.sampler.get()
            for _ in range(self.num_epochs):
                self.algorithm.learn(experience_batch)
        else:
            return self.algorithm.get_update_info(experience_batch)

    def get_state(self):
        """Return the current state of the policy.

        The implementation must be in correspondence with that of ``set_state``. For example, if a torch model
        is contained in the policy, ``get_state`` may include a call to ``state_dict()`` on the model, while
        ``set_state`` should accordingly include ``load_state_dict()``.
        """
        return self.algorithm.get_state()

    def set_state(self, state):
        """Set the policy state to ``policy_state``.

        The implementation must be in correspondence with that of ``get_state``. For example, if a torch model
        is contained in the policy, ``set_state`` may include a call to ``load_state_dict()`` on the model, while
        ``get_state`` should accordingly include ``state_dict()``.
        """
        self.algorithm.set_state(state)

    def get_update_info(self, experience_batch: ExperienceSet):
        pass

    def apply(self, update_info):
        if self.auto:
            raise 

        self.algorithm.apply(update_info)

    def store(self, exp: ExperienceSet) -> bool:
        """
        Store incoming experiences and update if necessary.
        """
        self.experience_store.put(exp)
        # print(
        #     f"exp mem size = {self.experience_store.size}, incoming: {exp.size}, new exp = {self._new_exp_counter}"
        # )

    def explore(self):
        self.algorithm = False

    def exploit(self):
        self.exploring = True

    def exploration_step(self):
        if self.algorithm.exploration:
            self.algorithm.exploration.step()

    def load(self, path: str):
        """Load the policy state from disk."""
        self.algorithm.load(path)

    def save(self, path: str):
        """Save the policy state to disk."""
        self.algorithm.save(path)
